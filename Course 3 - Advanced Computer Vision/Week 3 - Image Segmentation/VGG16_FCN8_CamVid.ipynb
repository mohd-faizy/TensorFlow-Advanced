{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG16-FCN8-CamVid.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSFfcSDxhIS7"
      },
      "source": [
        "#Fully Convolutional Neural Networks for Image Segmentation\n",
        "\n",
        "This notebook illustrates how to build an Fully Convolutional Neural Network for image segmentation(.\n",
        "\n",
        "We use a [custom dataset](https://drive.google.com/file/d/0B0d9ZiqAgFkiOHR1NTJhWVJMNEU/view?usp=sharing) prepared by [divamgupta](https://github.com/divamgupta/image-segmentation-keras).\n",
        "\n",
        "This dataset is a subsample of the [CamVid](http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/) dataset with only label maps for 12 classes out of the original 32 classes in the dataset.\n",
        "CamVid dataset is a video dataset with labelled object semantic classes. \n",
        "The custom dataset consists of labelled frames from some of the videos in the CamVid dataset.\n",
        "\n",
        "We are using a pretrained VGG-16 network as the starting point for the feature extraction path and FCN 8 upsampling for generating labelmaps from downsampled festure maps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ3g9dJxSxmN"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aifz2907kxYN"
      },
      "source": [
        "import os, re, time, json\n",
        "import zipfile\n",
        "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
        "import numpy as np\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"Tensorflow version \" + tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m9CxxThS1dg"
      },
      "source": [
        "##Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvjweKXoe4mI"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "class_names = ['sky', 'building','column/pole', 'road', 'side walk', 'vegetation', 'traffic light', 'fence', 'vehicle', 'pedestrian', 'byciclist', 'void']\n",
        "colors = sns.color_palette(None, len(class_names))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sLz8mI2S62W"
      },
      "source": [
        "##Download Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwC8gfP6jTZC"
      },
      "source": [
        "The dataset we are using is hosted in Google Drive. The following code snippets perform download files from Google Driveto our root folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z5V1XMBNJso"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/fcnn-dataset.zip \\\n",
        "    -O /tmp/fcnn-dataset.zip\n",
        "\n",
        "local_zip = '/tmp/fcnn-dataset.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/fcnn')\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi0NcGqESgj2"
      },
      "source": [
        "##Load and Prepare Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsRr8WWIjwTg"
      },
      "source": [
        "This dataset has images and label maps.\n",
        "Label maps are of size **(height, width , 1)** with each location denoting the corresponding pixel's class. Classes can be in the range **[0, 12]**\n",
        "\n",
        "In the following function we will map these pixel maps to **(height, width, 12)** with each slice along the third axis having 1 if it belongs to class corresponding to that slice's index else 0.\n",
        "\n",
        "We also resize images to a specific size and perform **normalization** on images so that pixels fall in the range **[-1, 1]**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lsc-_7Xu_twj"
      },
      "source": [
        "'''\n",
        "This function maps image and segmentation masks. Images are normalized so that each pixel is in the range [-1, 1]. Images and segmentation masks are resized.\n",
        "Segmentation masks are mapped from (height, width, 1) to (height, width, 12).\n",
        "'''\n",
        "def map_filename_to_image_and_mask(t_filename, a_filename, height=224, width=224):\n",
        "  img_raw = tf.io.read_file(t_filename)\n",
        "  anno_raw = tf.io.read_file(a_filename)\n",
        "  image = tf.image.decode_jpeg(img_raw)\n",
        "  annotation = tf.image.decode_jpeg(anno_raw)\n",
        " \n",
        "  #Resize Image and segmentation mask\n",
        "  image = tf.image.resize(image, (height, width,))\n",
        "  annotation = tf.image.resize(annotation, (height, width,))\n",
        "  image = tf.reshape(image, (height, width, 3,))\n",
        "  annotation = tf.cast(annotation, dtype=tf.int32)\n",
        "  annotation = tf.reshape(annotation, (height, width, 1,))\n",
        "  stack_list = []\n",
        "\n",
        "  #Map Segmentation Masks\n",
        "  for c in range(len(class_names)):\n",
        "    mask = tf.equal(annotation[:,:,0], tf.constant(c))\n",
        "    stack_list.append(tf.cast(mask, dtype=tf.int32))\n",
        "  \n",
        "  annotation = tf.stack(stack_list, axis=2)\n",
        "\n",
        "  #Normalize Images\n",
        "  image = image/127.5\n",
        "  image -= 1\n",
        "\n",
        "  return image, annotation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK9tYK28lUhx"
      },
      "source": [
        "Dataset has training and testing images.\n",
        "Training Dataset has two folders, one corresponding to images, other to annotaions. Similarly test dataset also has two folders. We create the tensorflow datasets from the images in these folders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8YE6w9g-ZEF"
      },
      "source": [
        "def get_dataset_slice_paths(image_dir, label_map_dir):\n",
        "  image_file_list = os.listdir(image_dir)\n",
        "  label_map_file_list = os.listdir(label_map_dir)\n",
        "  image_paths = [os.path.join(image_dir, fname) for fname in image_file_list]\n",
        "  label_map_paths = [os.path.join(label_map_dir, fname) for fname in label_map_file_list]\n",
        "\n",
        "  return image_paths, label_map_paths\n",
        "\n",
        "def get_training_dataset(image_paths, label_map_paths):\n",
        "  training_dataset = tf.data.Dataset.from_tensor_slices((image_paths, label_map_paths))\n",
        "  training_dataset = training_dataset.map(map_filename_to_image_and_mask)\n",
        "  training_dataset = training_dataset.shuffle(100, reshuffle_each_iteration=True)\n",
        "  training_dataset = training_dataset.batch(BATCH_SIZE)\n",
        "  training_dataset = training_dataset.repeat()\n",
        "  training_dataset = training_dataset.prefetch(-1)\n",
        "  return training_dataset\n",
        "\n",
        "def get_validation_dataset(image_paths, label_map_paths):\n",
        "  validation_dataset = tf.data.Dataset.from_tensor_slices((image_paths, label_map_paths))\n",
        "  validation_dataset = validation_dataset.map(map_filename_to_image_and_mask)\n",
        "  validation_dataset = validation_dataset.batch(BATCH_SIZE)\n",
        "  validation_dataset = validation_dataset.repeat()  \n",
        "  return validation_dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skVGwEPmeiwz"
      },
      "source": [
        "train_count = 367\n",
        "validation_count = 101\n",
        "training_image_paths, training_label_map_paths = get_dataset_slice_paths('/tmp/fcnn/dataset1/images_prepped_train/','/tmp/fcnn/dataset1/annotations_prepped_train/')\n",
        "validation_image_paths, validation_label_map_paths = get_dataset_slice_paths('/tmp/fcnn/dataset1/images_prepped_test/','/tmp/fcnn/dataset1/annotations_prepped_test/')\n",
        "\n",
        "training_dataset = get_training_dataset(training_image_paths, training_label_map_paths)\n",
        "validation_dataset = get_validation_dataset(validation_image_paths, validation_label_map_paths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d46YCbvPafbp",
        "cellView": "both"
      },
      "source": [
        "#@title Plot Utilities [RUN ME]\n",
        "def fuse_with_pil(images):\n",
        "  widths = (image.shape[1] for image in images)\n",
        "  heights = (image.shape[0] for image in images)\n",
        "  total_width = sum(widths)\n",
        "  max_height = max(heights)\n",
        "\n",
        "  new_im = PIL.Image.new('RGB', (total_width, max_height))\n",
        "\n",
        "  x_offset = 0\n",
        "  for im in images:\n",
        "    pil_image = PIL.Image.fromarray(np.uint8(im))\n",
        "    new_im.paste(pil_image, (x_offset,0))\n",
        "    x_offset += im.shape[1]\n",
        "  \n",
        "  return new_im\n",
        "\n",
        "def give_color_to_annotation(annotation):\n",
        "  seg_img = np.zeros( (annotation.shape[0],annotation.shape[1],3) ).astype('float')\n",
        "  for c in range(12):\n",
        "    segc = (annotation == c)\n",
        "    seg_img[:,:,0] += segc*( colors[c][0] * 255.0)\n",
        "    seg_img[:,:,1] += segc*( colors[c][1] * 255.0)\n",
        "    seg_img[:,:,2] += segc*( colors[c][2] * 255.0)\n",
        "  return seg_img\n",
        "\n",
        "def show_predictions(image, labelmaps, titles, iou_list, dice_score_list):\n",
        "  true_img = give_color_to_annotation(labelmaps[1])\n",
        "  pred_img = give_color_to_annotation(labelmaps[0])\n",
        "\n",
        "\n",
        "  image = image + 1\n",
        "  image = image * 127.5\n",
        "  images = np.uint8([image, pred_img, true_img])\n",
        "\n",
        "  metrics_by_id = [(idx, iou, dice_score) for idx, (iou, dice_score) in enumerate(zip(iou_list, dice_score_list)) if iou > 0.0]\n",
        "  metrics_by_id.sort(key=lambda tup: tup[1], reverse=True)  # sorts in place\n",
        "  \n",
        "  display_string_list = [\"{}: IOU: {} Dice Score: {}\".format(class_names[idx], iou, dice_score) for idx, iou, dice_score in metrics_by_id]\n",
        "  display_string = \"\\n\\n\".join(display_string_list) \n",
        "\n",
        "  plt.figure(figsize=(15, 4))\n",
        "\n",
        "  for idx, im in enumerate(images):\n",
        "    plt.subplot(1, 3, idx+1)\n",
        "    if idx == 1:\n",
        "      plt.xlabel(display_string)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.title(titles[idx], fontsize=12)\n",
        "    plt.imshow(im)\n",
        "\n",
        "\n",
        "def show_annotation_and_image(image, annotation):\n",
        "  new_ann = np.argmax(annotation, axis=2)\n",
        "  seg_img = give_color_to_annotation(new_ann)\n",
        "  \n",
        "  image = image + 1\n",
        "  image = image * 127.5\n",
        "  image = np.uint8(image)\n",
        "  images = [image, seg_img]\n",
        "  \n",
        "  images = [image, seg_img]\n",
        "  fused_img = fuse_with_pil(images)\n",
        "  plt.imshow(fused_img)\n",
        "\n",
        "\n",
        "def list_show_annotation(dataset):\n",
        "  ds = dataset.unbatch()\n",
        "  ds = ds.shuffle(buffer_size=100)\n",
        "\n",
        "  plt.figure(figsize=(25, 15))\n",
        "  plt.title(\"Images And Annotations\")\n",
        "  plt.subplots_adjust(bottom=0.1, top=0.9, hspace=0.05)\n",
        "\n",
        "  for idx, (image, annotation) in enumerate(ds.take(9)):\n",
        "    plt.subplot(3, 3, idx + 1)\n",
        "    plt.yticks([])\n",
        "    plt.xticks([])\n",
        "    show_annotation_and_image(image.numpy(), annotation.numpy())\n",
        "\n",
        "# utility to display training and validation curves\n",
        "def plot_metrics(metric_name, title, ylim=5):\n",
        "  plt.title(title)\n",
        "  plt.ylim(0,ylim)\n",
        "  plt.plot(history.history[metric_name],color='blue',label=metric_name)\n",
        "  plt.plot(history.history['val_' + metric_name],color='green',label='val_' + metric_name)\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK621Jm8bJyj"
      },
      "source": [
        "##Let's Take a Look at the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFO_hIhLWYT4"
      },
      "source": [
        "list_show_annotation(training_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdgVkp8wZua0"
      },
      "source": [
        "list_show_annotation(validation_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFv2k8xabRb8"
      },
      "source": [
        "##Define the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lm_8Jp4PbVV5"
      },
      "source": [
        "###Download vgg weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKPpXapoYxAc"
      },
      "source": [
        "!wget https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
        "vgg_weights_path = \"vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHB1BGmF965d"
      },
      "source": [
        "###Define Pooling Block of VGG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWnI0IcNxNRh"
      },
      "source": [
        "Each block in the VGG encoder has convolutional layers followed by max pooling layer which downsamples the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pL578pjdmXXf"
      },
      "source": [
        "'''\n",
        "This functions defines a block in the VGG network.\n",
        "'''\n",
        "def block(x, n_convs, filters, kernel_size, activation, pool_size, pool_stride, block_name):\n",
        "  for i in range(n_convs):\n",
        "      x = tf.keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, activation=activation, padding='same', name=\"{}_conv{}\".format(block_name, i + 1))(x)\n",
        "  x = tf.keras.layers.MaxPooling2D(pool_size=pool_size, strides=pool_stride, name=\"{}_pool{}\".format(block_name, i+1 ))(x)\n",
        "  return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLeQCxf99_tn"
      },
      "source": [
        "###Define VGG-16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4_WZnAmoOnZ"
      },
      "source": [
        "'''\n",
        "This function defines the VGG encoder.\n",
        "'''\n",
        "def VGG_16(image_input):\n",
        "  x = block(image_input,n_convs=2, filters=64, kernel_size=(3,3), activation='relu',pool_size=(2,2), pool_stride=(2,2), block_name='block1')\n",
        "  p1= x\n",
        "\n",
        "  x = block(x,n_convs=2, filters=128, kernel_size=(3,3), activation='relu',pool_size=(2,2), pool_stride=(2,2), block_name='block2')\n",
        "  p2 = x\n",
        "\n",
        "  x = block(x,n_convs=3, filters=256, kernel_size=(3,3), activation='relu',pool_size=(2,2), pool_stride=(2,2), block_name='block3')\n",
        "  p3 = x\n",
        "\n",
        "  x = block(x,n_convs=3, filters=512, kernel_size=(3,3), activation='relu',pool_size=(2,2), pool_stride=(2,2), block_name='block4')\n",
        "  p4 = x\n",
        "\n",
        "  x = block(x,n_convs=3, filters=512, kernel_size=(3,3), activation='relu',pool_size=(2,2), pool_stride=(2,2), block_name='block5')\n",
        "  p5 = x\n",
        "\n",
        "  vgg  = tf.keras.Model(image_input , p5)\n",
        "  vgg.load_weights(vgg_weights_path) \n",
        "\n",
        "  n = 4096\n",
        "  c6 = tf.keras.layers.Conv2D( n , ( 7 , 7 ) , activation='relu' , padding='same', name=\"conv6\")(p5)\n",
        "  c7= tf.keras.layers.Conv2D( n , ( 1 , 1 ) , activation='relu' , padding='same', name=\"conv7\")(c6)\n",
        "  return(p1, p2, p3, p4, c7)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45pH17d__KUW"
      },
      "source": [
        "###Define FCN 8 Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX2V0E2gs-ZQ"
      },
      "source": [
        "'''\n",
        "This function defines the FCN 8 decoder.\n",
        "'''\n",
        "def fcn8_decoder(convs, n_classes):\n",
        "  f1, f2, f3, f4, f5 = convs\n",
        "  \n",
        "  o = tf.keras.layers.Conv2DTranspose(n_classes , kernel_size=(4,4) ,  strides=(2,2) , use_bias=False )(f5)\n",
        "  o = tf.keras.layers.Cropping2D(cropping=(1,1))(o)\n",
        "\n",
        "  o2 = f4\n",
        "  o2 = ( tf.keras.layers.Conv2D(n_classes , ( 1 , 1 ) , activation='relu' , padding='same'))(o2)\n",
        "\n",
        "  o = tf.keras.layers.Add()([o, o2])\n",
        "\n",
        "  o = (tf.keras.layers.Conv2DTranspose( n_classes , kernel_size=(4,4) ,  strides=(2,2) , use_bias=False ))(o)\n",
        "\n",
        "  o2 = ( tf.keras.layers.Conv2D(n_classes , ( 1 , 1 ) , activation='relu' , padding='same'))(f3)\n",
        "\n",
        "  o = tf.keras.layers.Cropping2D(cropping=(1, 1))(o)\n",
        "  o = tf.keras.layers.Add()([o, o2])\n",
        "     \n",
        "  o = tf.keras.layers.Conv2DTranspose(n_classes , kernel_size=(8,8) ,  strides=(8,8) , use_bias=False )(o)\n",
        "\n",
        "  o = (tf.keras.layers.Activation('softmax'))(o)\n",
        "\n",
        "\n",
        "  return o"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyn3xXSf_Ogl"
      },
      "source": [
        "###Define Final Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T29n8_dbuZNm"
      },
      "source": [
        "'''\n",
        "This function defines the final segmentation model by chaining together the encoder and decoder.\n",
        "'''\n",
        "def segmentation_model():\n",
        "  inputs = tf.keras.layers.Input(shape=(224,224,3,))\n",
        "  convs = VGG_16(image_input=inputs)\n",
        "  outputs = fcn8_decoder(convs, 12)\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w8qNGG1vQHZ"
      },
      "source": [
        "model = segmentation_model()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dekOKLw0_Rgg"
      },
      "source": [
        "###Compile Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cAcCHkyxqt4"
      },
      "source": [
        "We use categorical crossentropy since we have transformed the label map to one hot encoded vectors for each pizel in the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpWpp8h4g_rE"
      },
      "source": [
        "sgd = tf.keras.optimizers.SGD(lr=1E-2, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9zxLlNZ_XbT"
      },
      "source": [
        "##Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HoZwpGWhMB-"
      },
      "source": [
        "EPOCHS = 200\n",
        "steps_per_epoch = train_count//BATCH_SIZE\n",
        "validation_steps = validation_count//BATCH_SIZE\n",
        "\n",
        "history = model.fit(training_dataset,\n",
        "                    steps_per_epoch=steps_per_epoch, validation_data=validation_dataset, validation_steps=validation_steps, epochs=EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1luX1e7_bEd"
      },
      "source": [
        "#Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zENjQuK0luH5"
      },
      "source": [
        "def get_images_and_segments_test_arrays():\n",
        "  y_true_segments = []\n",
        "  y_true_images = []\n",
        "  test_count = 64\n",
        "\n",
        "  ds = validation_dataset.unbatch()\n",
        "  ds = ds.batch(101)\n",
        "\n",
        "  for image, annotation in ds.take(1):\n",
        "    y_true_images = image\n",
        "    y_true_segments = annotation\n",
        "\n",
        "\n",
        "  y_true_segments = y_true_segments[:test_count, : ,: , :]\n",
        "  y_true_segments = np.argmax(y_true_segments, axis=3)  \n",
        "\n",
        "  return y_true_images, y_true_segments\n",
        "\n",
        "y_true_images, y_true_segments = get_images_and_segments_test_arrays()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly1ErxSA_kpb"
      },
      "source": [
        "###Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CpEeUiN7ey9"
      },
      "source": [
        "results = model.predict(validation_dataset, steps=validation_steps)\n",
        "results = np.argmax(results, axis=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-nmUQkp_dc6"
      },
      "source": [
        "###Compute IOU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EobztGe_66sA"
      },
      "source": [
        "'''\n",
        "This function computes IOU and Dice Score.\n",
        "'''\n",
        "def compute_metrics(y_true, y_pred):\n",
        "  class_wise_iou = []\n",
        "  class_wise_dice_score = []\n",
        "\n",
        "  smoothening_factor = 0.00001\n",
        "\n",
        "  for i in range(12):\n",
        "    intersection = np.sum((y_pred == i) * (y_true == i))\n",
        "    y_true_area = np.sum((y_true == i))\n",
        "    y_pred_area = np.sum((y_pred == i))\n",
        "    combined_area = y_true_area + y_pred_area\n",
        "    \n",
        "    iou = (intersection + smoothening_factor) / (combined_area - intersection + smoothening_factor)\n",
        "    class_wise_iou.append(iou)\n",
        "    \n",
        "    dice_score =  2 * ((intersection + smoothening_factor) / (combined_area + smoothening_factor))\n",
        "    class_wise_dice_score.append(dice_score)\n",
        "\n",
        "  return class_wise_iou, class_wise_dice_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duS-cSFMy1VH"
      },
      "source": [
        "###Show Predictions and IOU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hkbsk_P1fpRM",
        "cellView": "both"
      },
      "source": [
        "#@title Visualize Output [RUN ME]\n",
        "integer_slider = 63 #@param {type:\"slider\", min:0, max:63, step:1}\n",
        "iou, dice_score = compute_metrics(y_true_segments[integer_slider], results[integer_slider])  \n",
        "show_predictions(y_true_images[integer_slider], [results[integer_slider], y_true_segments[integer_slider]], [\"Image\", \"Predicted Mask\", \"True Mask\"], iou, dice_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psu5U4jRy5px"
      },
      "source": [
        "###Display Class Wise Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqzDRh0e6_8G"
      },
      "source": [
        "cls_wise_iou, cls_wise_dice_score = compute_metrics(y_true_segments, results)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mnS0UPtsMeB"
      },
      "source": [
        "for idx, iou in enumerate(cls_wise_iou):\n",
        "  spaces = ' ' * (13-len(class_names[idx]) + 2)\n",
        "  print(\"{}{}{} \".format(class_names[idx], spaces, iou)) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVxYk02pJm8O"
      },
      "source": [
        "for idx, dice_score in enumerate(cls_wise_dice_score):\n",
        "  spaces = ' ' * (13-len(class_names[idx]) + 2)\n",
        "  print(\"{}{}{} \".format(class_names[idx], spaces, dice_score)) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTSQrmQiKu1L"
      },
      "source": [
        "###Plot Loss Curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ntpo_5xZKa29"
      },
      "source": [
        "plot_metrics(\"loss\", title=\"Training vs Validation Loss\", ylim=2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}