{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Extending TensorFlow and Keras Part2 Module 2-Exercise(Answer).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWqcoPhU3RJN",
        "colab_type": "text"
      },
      "source": [
        "#Breast Cancer Prediction\n",
        "This exercise will be to train a neural network on the [Breast Cancer Dataset](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original)) to predict if the tumor is malignant or benign "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st5AIBFZ5mEQ",
        "colab_type": "text"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkMXve8XuN5X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "  \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mticker\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "import tensorflow_datasets as tfds\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUc3HpEQ5s6U",
        "colab_type": "text"
      },
      "source": [
        "##Load and Preprocess the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-TQFUXu5wS_",
        "colab_type": "text"
      },
      "source": [
        "We first download the dataset and create a data frame using pandas. We explicitly specify the column names because the csv file does not have column headers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVh-W73J5TjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATASET_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\"\n",
        "data_file = tf.keras.utils.get_file(\"breast_cancer.csv\", DATASET_URL)\n",
        "col_names = [\"id\", \"clump_thickness\", \"un_cell_size\", \"un_cell_shape\", \"marginal_adheshion\", \"single_eph_cell_size\", \"bare_nuclei\", \"bland_chromatin\", \"normal_nucleoli\", \"mitoses\", \"class\"]\n",
        "df = pd.read_csv(data_file, names=col_names, header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEv8vS_P6HaV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvvbnFL36L85",
        "colab_type": "text"
      },
      "source": [
        "We have to do some preprocessing on the data. We first pop the id column since it is of no use for our problem at hand."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDeXwHdA5uUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.pop(\"id\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubw5LueA6ZEY",
        "colab_type": "text"
      },
      "source": [
        "Upon inspection of data, you can see that some values of the **bare_nuclei** column are unknown. We drop the rows with these unknown values. We also convert bare_nuclei column to numeric. This is required for training of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCcOrl1ITVhr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df[df[\"bare_nuclei\"] != '?' ]\n",
        "df.bare_nuclei = pd.to_numeric(df.bare_nuclei)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQMhcTQG7LzY",
        "colab_type": "text"
      },
      "source": [
        "We check the class distribution of the data. You can see that there are two classes, 2.0 and 4.0\n",
        "According to the dataset:\n",
        "* **2.0 = benign**\n",
        "* **4.0 = malignant**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaAdQrBv8daS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['class'].hist(bins=20) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENjMKvxQ6sWy",
        "colab_type": "text"
      },
      "source": [
        "We are going to model this problem as a binary classification problem which detects whether the tumor is malignant or not. Hence we change the dataset so that:\n",
        "* **benign(2.0) = 0**\n",
        "* **malignant(4.0) = 1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MVzeUwf_A3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['class'] = np.where(df['class'] == 2, 0, 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGbKO1bR8S9h",
        "colab_type": "text"
      },
      "source": [
        "We then split the dataset into training and testing sets. Since the number of samples are less, we will perform validation on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNUy7JcuAXjC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test = train_test_split(df, test_size = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_ZKokUP8kP3",
        "colab_type": "text"
      },
      "source": [
        "We get the statistics for training. We can look at statistics to gather an idea about the distribution of plots. If you need more visualization, you can create additional data plots. We will also be using the mean and standard deviation from statistics for normalizing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k86tBT_QAm2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_stats = train.describe()\n",
        "train_stats.pop('class')\n",
        "train_stats = train_stats.transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8AJ0Crc8u9t",
        "colab_type": "text"
      },
      "source": [
        "We pop the class column from the training and test sets to create train and test outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7EGUV-tA5LZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_Y = train.pop(\"class\")\n",
        "test_Y = test.pop(\"class\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9wVRO5E9AgA",
        "colab_type": "text"
      },
      "source": [
        "Here we normalize the data by using the formula: **X = X - mean(X) / StandardDeviation(X)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDo__q_AA3j0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def norm(x):\n",
        "  return (x - train_stats['mean']) / train_stats['std']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdARlWaDA_8G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "norm_train_X = norm(train)\n",
        "norm_test_X = norm(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6LIVZbj9Usv",
        "colab_type": "text"
      },
      "source": [
        "We now create tensorflow datasets for training and test sets so as to easily be able to build and manage an input pipeline for our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S0RtsP1Xsj8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((norm_train_X.values, train_Y.values))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((norm_test_X.values, test_Y.values))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Nb44PpV9hR4",
        "colab_type": "text"
      },
      "source": [
        "We shuffle and prepare batched dataset to be used for training in our custom training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9qdsNPen5-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "train_dataset = train_dataset.shuffle(buffer_size=len(train)).batch(batch_size)\n",
        "\n",
        "test_dataset =  test_dataset.batch(batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcbOJ6C79qT5",
        "colab_type": "text"
      },
      "source": [
        "##Define the Model\n",
        "Now we will define the model. Here we use Keras Functional API to create a simple network of two dense layers. We have modelled the problem as a binary classification problem and hence we add a single layer with sigmoid activation as the final layer of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU3qcM9WBcMh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def base_model():\n",
        "  inputs = tf.keras.layers.Input(shape=(len(train.columns)))\n",
        "\n",
        "  x = tf.keras.layers.Dense(128, activation='relu')(inputs)\n",
        "  x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
        "  outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "  return model\n",
        "\n",
        "model = base_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBhKIcKQ-Bwe",
        "colab_type": "text"
      },
      "source": [
        "##Define Optimizer and Loss\n",
        "\n",
        "We use RMSprop optimizer and binary crossentropy as our loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5B3vh6fs84i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "loss_object = tf.keras.losses.BinaryCrossentropy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSNDewgovSZ8",
        "colab_type": "text"
      },
      "source": [
        "##Evaluate Untrained Model\n",
        "We calculate the loss on the model before training begins."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUScS3GbtPXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outputs = model(norm_test_X.values)\n",
        "loss_value = loss_object(y_true=test_Y.values, y_pred=outputs)\n",
        "print(\"Loss before training %.4f\" % loss_value.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPPb5ewkzMBY",
        "colab_type": "text"
      },
      "source": [
        "We also plot the confusion matrix to visualize the true outputs against the outputs predicted byb the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueenYwWZvQM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, title='', labels=[0,1]):\n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_subplot(111)\n",
        "  cax = ax.matshow(cm)\n",
        "  plt.title('Confusion matrix of the classifier')\n",
        "  fig.colorbar(cax)\n",
        "  ax.set_xticklabels([''] + labels)\n",
        "  ax.set_yticklabels([''] + labels)\n",
        "  plt.xlabel('Predicted')\n",
        "  plt.ylabel('True')\n",
        "  fmt = 'd'\n",
        "  thresh = cm.max() / 2.\n",
        "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "          plt.text(j, i, format(cm[i, j], fmt),\n",
        "                  horizontalalignment=\"center\",\n",
        "                  color=\"black\" if cm[i, j] > thresh else \"white\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FApnBUNWv-ZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_confusion_matrix(test_Y.values, tf.round(outputs), title='Confusion Matrix for Untrained Model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-HTkbQb-gYp",
        "colab_type": "text"
      },
      "source": [
        "##Define Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYUyRka1-j87",
        "colab_type": "text"
      },
      "source": [
        "##Define Custom F1Score Metric\n",
        "In this example, we will define a custom F1Score metric using the formula. \n",
        "\n",
        "**F1 Score = 2 * ((precision * recall) / (precision + recall))**\n",
        "\n",
        "**precision = true_positives / (true_positives + false_positives)**\n",
        "**recall = true_positives / (true_positives + false_negatives)**\n",
        "\n",
        "We use `confusion_matrix` defined in `tf.math` to calculate precision and recall.\n",
        "\n",
        "Here you can see that we have subclassed `tf.keras.Metric` and implemented the three required methods `update_state`, `result` and `reset_states`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdUe6cqvbzXy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class F1Score(tf.keras.metrics.Metric):\n",
        "\n",
        "    def __init__(self, name='f1_score', **kwargs):\n",
        "      super(F1Score, self).__init__(name=name, **kwargs)\n",
        "      \n",
        "      #Initialize Required variables\n",
        "      self.tp, self.fp, self.tn, self.fn = tf.Variable(0, dtype = 'int32'),tf.Variable(0, dtype='int32'),tf.Variable(0, dtype='int32'),tf.Variable(0, dtype='int32')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "\n",
        "      #Calulcate confusion matrix.\n",
        "      conf_matrix = tf.math.confusion_matrix(y_true, y_pred, num_classes=2)\n",
        "      #Update values of true positives, true negatives, false positives and false negatives from confusion matrix.\n",
        "      self.tn.assign_add(conf_matrix[0][0])\n",
        "      self.tp.assign_add(conf_matrix[1][1])\n",
        "      self.fp.assign_add(conf_matrix[0][1])\n",
        "      self.fn.assign_add(conf_matrix[1][0])\n",
        "\n",
        "    def result(self):\n",
        "\n",
        "      #Calculate precision\n",
        "      if (self.tp + self.fp == 0):\n",
        "        precision = 1.0\n",
        "      else:\n",
        "        precision = self.tp / (self.tp + self.fp)\n",
        "      \n",
        "      #Calculate recall\n",
        "      if (self.tp + self.fn == 0):\n",
        "        recall = 1.0\n",
        "      else:\n",
        "        recall = self.tp / (self.tp + self.fn)\n",
        "\n",
        "      #Return F1 Score\n",
        "      return 2.0 * ((precision * recall) / (precision + recall))\n",
        "\n",
        "    def reset_states(self):\n",
        "      # The state of the metric will be reset at the start of each epoch.\n",
        "      self.tp.assign(0)\n",
        "      self.tn.assign(0) \n",
        "      self.fp.assign(0)\n",
        "      self.fn.assign(0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiTa2CePAOTa",
        "colab_type": "text"
      },
      "source": [
        "We initialize the seprate metrics required for training and validation. In addition to our custom F1Score metric we are also using `BinaryAccuracy` defined in `tf.keras.metrics`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Pa_x-5-CH_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_f1score_metric = F1Score()\n",
        "val_f1score_metric = F1Score()\n",
        "\n",
        "train_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
        "val_acc_metric = tf.keras.metrics.BinaryAccuracy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5nquSQuAg50",
        "colab_type": "text"
      },
      "source": [
        "##Build Training Loop\n",
        "In this section we build our training loop consisting of training and validation sequences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1huOxRpEAxvf",
        "colab_type": "text"
      },
      "source": [
        "The core of training is using the model to calculate the logits on specific set of inputs and compute loss(in this case **binary crossentropy**) by comparing the predicted outputs to the true outputs. We then update the trainable weights using the optimizer algorithm chosen. Optimizer algorithm requires our computed loss and partial derivatives of loss with respect to each of the trainable weights to make updates to the same.\n",
        "\n",
        "We use gradient tape to calculate the gradients and then update the model trainable weights using the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMPe25Dstn0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def apply_gradient(optimizer, model, x, y):\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = model(x)\n",
        "    loss_value = loss_object(y_true=y, y_pred=logits)\n",
        "  \n",
        "  gradients = tape.gradient(loss_value, model.trainable_weights)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "  \n",
        "  return logits, loss_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYM6GZPjB40r",
        "colab_type": "text"
      },
      "source": [
        "This function performs training during one epoch. We run through all batches of training data in each epoch to make updates to trainable weights using our previous function.\n",
        "You can see that we also call update_state on our metrics to accumulate the value of our metrics. \n",
        "\n",
        "We are displaying a progress bar to indicate completion of training in each epoch. Here we use tqdm for displaying the progress bar. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fHoh_hgz2PC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_data_for_one_epoch():\n",
        "  losses = []\n",
        "  \n",
        "  #Iterate through all batches of training data\n",
        "  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "      \n",
        "      #Calculate loss and update trainable variables using optimizer\n",
        "      logits, loss_value = apply_gradient(optimizer, model, x_batch_train, y_batch_train)\n",
        "      losses.append(loss_value)\n",
        "\n",
        "      #Round off logits to nearest integer and cast to integer for calulating metrics\n",
        "      logits = tf.round(logits)\n",
        "      logits = tf.cast(logits, 'int64')\n",
        "      \n",
        "      #Update the training metrics\n",
        "      train_acc_metric.update_state(y_batch_train, logits)\n",
        "      train_f1score_metric.update_state(y_batch_train, logits)\n",
        "      \n",
        "      #Update progress\n",
        "      print(\"Training loss for step %s: %.4f\" % (int(step), float(loss_value)))\n",
        "  return losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9RJq8BLCsSF",
        "colab_type": "text"
      },
      "source": [
        "At the end of each epoch we have to validate the model on the test dataset. The following function calculates the loss on test dataset and updates the states of the validation metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gLJyAJE0YRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def perform_validation():\n",
        "  losses = []\n",
        "\n",
        "  #Iterate through all batches of validation data.\n",
        "  for x_val, y_val in test_dataset:\n",
        "\n",
        "      #Calculate validation loss for current batch.\n",
        "      val_logits = model(x_val) \n",
        "      val_loss = loss_object(y_true=y_val, y_pred=val_logits)\n",
        "      losses.append(val_loss)\n",
        "      \n",
        "      #Round off and cast outputs to either  or 1\n",
        "      val_logits = tf.cast(tf.round(model(x_val)), 'int64')\n",
        "       \n",
        "      #Update validation metrics\n",
        "      val_acc_metric.update_state(y_val, val_logits)\n",
        "      val_f1score_metric.update_state(y_val, val_logits)\n",
        "  return losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLymSCkUC-CL",
        "colab_type": "text"
      },
      "source": [
        "Next we define the training loop that runs through the training samples repeatedly over a fixed number of epochs. Here we combine the functions we built earlier to establish the following flow:\n",
        "1. Perform training over all batches of training data.\n",
        "2. Get values of metrics.\n",
        "3. Perform validation to calculate loss and update validation metrics on test data.\n",
        "4. Reset the metrics at the end of epoch.\n",
        "5. Display statistics at the end of each epoch.\n",
        "\n",
        "**Note** : We also calculate the training and validation losses for the whole epoch at the end of the epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOO1x3VyuPUV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Iterate over epochs.\n",
        "epochs = 5\n",
        "epochs_val_losses, epochs_train_losses = [], []\n",
        "for epoch in range(epochs):\n",
        "  print('Start of epoch %d' % (epoch,))\n",
        "  #Perform Training over all batches of train data\n",
        "  losses_train = train_data_for_one_epoch()\n",
        "  \n",
        "  # Get results from training metrics\n",
        "  train_acc = train_acc_metric.result()\n",
        "  train_f1score = train_f1score_metric.result()\n",
        "\n",
        "  #Perform validation on all batches of test data\n",
        "  losses_val = perform_validation()\n",
        "  \n",
        "  # Get results from validation metrics\n",
        "  val_acc = val_acc_metric.result()\n",
        "  val_f1score = val_f1score_metric.result()\n",
        "\n",
        "  #Calculate training and validation losses for current epoch\n",
        "  losses_train_mean = np.mean(losses_train)\n",
        "  losses_val_mean = np.mean(losses_val)\n",
        "  epochs_val_losses.append(losses_val_mean)\n",
        "  epochs_train_losses.append(losses_train_mean)\n",
        "  \n",
        "  print('\\n Epcoh %s: Train loss: %.4f  Validation Loss: %.4f, Train Accuracy: %.4f, Validation Accuracy %.4f, Train F1 Score: %.4f, Validation F1 Score: %.4f' % (epoch, float(losses_train_mean), float(losses_val_mean), float(train_acc), float(val_acc), train_f1score, val_f1score))\n",
        "  \n",
        "  #Reset states of all metrics\n",
        "  train_acc_metric.reset_states()\n",
        "  val_acc_metric.reset_states()\n",
        "  val_f1score_metric.reset_states()\n",
        "  train_f1score_metric.reset_states()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoLxueMdzm14",
        "colab_type": "text"
      },
      "source": [
        "##Evaluate Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EGW3HVUzqBX",
        "colab_type": "text"
      },
      "source": [
        "###Plots for Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8Wsr6wG0T4h",
        "colab_type": "text"
      },
      "source": [
        "We plot the progress of loss as training proceeds over number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsmF_2n307SP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_metrics(train_metric, val_metric, metric_name, title, ylim=5):\n",
        "  plt.title(title)\n",
        "  plt.ylim(0,ylim)\n",
        "  plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(1))\n",
        "  plt.plot(train_metric,color='blue',label=metric_name)\n",
        "  plt.plot(val_metric,color='green',label='val_' + metric_name)\n",
        "\n",
        "plot_metrics(epochs_train_losses, epochs_val_losses, \"Loss\", \"Loss\", ylim=1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27fXX7Yqyu5S",
        "colab_type": "text"
      },
      "source": [
        "We plot the confusion matrix to visualize the true values against the values predicted by the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9n2XJ9MwpDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_outputs = model(norm_test_X.values)\n",
        "plot_confusion_matrix(test_Y.values, tf.round(test_outputs), title='Confusion Matrix for Untrained Model')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}